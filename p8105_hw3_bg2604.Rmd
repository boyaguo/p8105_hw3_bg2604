---
title: "p8105_hw3_bg2604"
author: "Boya Guo"
date: "10/15/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
library(tidyverse)
library(ggridges)
library(hexbin)
devtools::install_github("thomasp85/patchwork")
theme_set(theme_bw())
```

### Problem 1

```{r p1 Import and clean dataset}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
p1_brfss=brfss_smart2010 %>%
  janitor::clean_names() %>%
  separate(locationdesc, into = c("state", "county"), sep = " - ") %>%
  filter(topic == 'Overall Health') %>%
  mutate(response = as.factor(response),
         response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>%
  select(-locationabbr, year, state, county, response, everything())
```
Answer: The names of variables are cleaned. Locationdesc is seperated into state and county, and Locationdesc is not selected in the final dataset. We then filtered only Overal Health topic and ordered responses from Excellent to Poor. 

#### Problem 1.1 In 2002, which states were observed at 7 locations?

```{r p1.1}
p1_brfss %>%
  filter(year == 2002) %>% 
  group_by(year, state) %>% 
  summarize(count = n_distinct(county)) %>% 
  filter(count == 7)
```
Answer: According to the R output, CT, FL and NC states were observed at 7 locations.

#### Problem 1.2 Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r p1.2}
p1_brfss %>% 
  group_by(year, state) %>% 
  summarize(count = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = count, color = state)) + 
  geom_point() + 
  geom_line(alpha = .5) + 
  labs( title = "The Number of locations in each state, 2002-2010",
    x = "Year",
    y = "Number of locations"
    ) + 
  theme(legend.position = "left")
```
Answer: We created a "spaghetti plot" shows the number of locations in each state from 2002 to 2010. From graph, we could see that for most states, the number of locations is below 20. However, Florida has two peak value above 40, one in 2007 and another in 2010. 

#### Problem 1.3 Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r p1.3}
brfss_p1.3 <- p1_brfss %>%
  spread(key = "response", value = "data_value") %>% 
  janitor::clean_names() %>%
  filter( year == 2002 | year == 2006 | year == 2010) %>%   
  filter(state == "NY") %>% 
  group_by(state, county) %>% 
  summarize(avg_excellent = mean(excellent, na.rm = TRUE),sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)

brfss_p1.3
```
Answer: The mean of the proportion of "Excellent" responses is the highest in New York County, and the lowest in Erie County. The standard deviation ranges from 0.6 to 2.8. Bronx, Erie and Monroe counties have NA values from standard deviation.

#### Problem 1.4 For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r p1.4}
p1_brfss %>% 
  spread(key = "response", value = "data_value") %>% 
  janitor::clean_names() %>% 
  select(year, state, county, excellent:poor) %>% 
  group_by(year, state) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            mean_very_good = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)
            ) %>% 
  gather(key = "mean_response", value = "average_proportion",    
  mean_excellent:mean_poor) %>%
  mutate(mean_response = as.factor(mean_response),
  mean_response = factor(mean_response, levels = c("mean_excellent", "mean_very_good", "mean_good", "mean_fair", "mean_poor"))) %>% 
  ggplot(aes(x = year, y = average_proportion, color = state)) + 
  geom_point() + geom_line() +
  facet_grid(~mean_response) +
  labs(
    title = "Average proportion in each overall health response for years and states",
    x = "Year",
    y = "Average proportion"
  ) + theme(legend.position = "none",axis.text.x = element_text(angle = 45))
```
Answer: Five-panel plot were created for earch response in each state across years. A similar trend was observed for each state for each response type from 2002 to 2010.

### Problem 2 

#### Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. 

```{r p2}
instacart
sum(is.na(instacart))
nrow(distinct(instacart, order_id))
nrow(distinct(instacart, user_id))
nrow(distinct(instacart, product_id))
```

Answer: This is a 1384617*15 dataset without missing data. There are 131209 orders, 131209 users and 39123 products. The variables include information (id and name) of departments, aisles and products, and order information (eg. order id, number of order, order day).

#### Problem 2.1 How many aisles are there, and which aisles are the most items ordered from?

```{r p2.1}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_item = n()) %>% 
  arrange(desc(n_item))
```
Answer: There are 134 aisles in the dataset. The fresh vegtables, fresh fruits, packaged vegetables fruits are the top three aisles with the most items ordered. 

#### Problem 2.2 Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r p2.2}
instacart_p2.2 <- instacart %>% 
  group_by(aisle) %>% 
  summarize(item_count = n()) %>% 
  mutate(group = as.numeric(cut_number(item_count, 3))) %>% 
  ggplot(aes(x = aisle, y = item_count)) +
    geom_point() +
    facet_wrap(group ~ ., nrow = 3, scales = "free") +
    theme(axis.text.x = element_text(size = 6, hjust = 1, angle = 45)) +
    labs(
      title = "Number of items ordered in aisles",
      x = "Aisle Name",
      y = "Number of Items Ordered"
      )
instacart_p2.2
```
Answer: We dividied the plot into 3 parts to make it more clear for readers to view. Fresh vegetables and fresh fruits have largest numbers of items ordered in aisles. Beauty products have the lowest number of items ordered. 

#### Problem 2.3 Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r p2.3}
instacart %>%  
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(count = n()) %>% 
  group_by(aisle) %>% 
  mutate(rank = min_rank(desc(count))) %>%  
  filter(rank < 2)
```
Answer: The most popular items in baking ingredients, dog food care, and packaged vegetables fruits are Light Brown Suger, Snack Sticks Chicken & Rice Recipe Dog Treats, and Organic Baby Spinach, respectively.

#### Problem 2.4 Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r p2.4}
instacart %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = (mean(order_hour_of_day)) ) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  spread(key = "product_name", value = "mean_hour") %>% 
  knitr::kable()
```
Answer: A table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week was created. 0 to 6 represents Sunday to Saturday in each week. The highest mean hour of the day at which Coffee Ice Cream are ordered is Tuesday (13.77), and Wednesday for Pinck Lady Apples (14.25). 

### Problem 3

#### Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue

```{r p3}
ny_noaa %>% 
  group_by(id) %>% 
  summarize(prcp_na = sum(is.na(prcp)),
            snow_na = sum(is.na(snow)),
            snwd_na = sum(is.na(snwd)),
            tmax_na = sum(is.na(tmax)),
            tmin_na = sum(is.na(tmin)))
```
Answer: This is a 2595176*7 dataset. The variables include NY weather station id, the date of observation, precipitation (mm), snowfall (mm), snow depth (mm), and the maximum and minimum temperatures (tenths of degrees C). Missing data could be a big problem becasue a large proportion of data is missing.

```{r p3.1}

```


